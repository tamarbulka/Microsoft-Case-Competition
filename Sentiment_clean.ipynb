{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GetOldTweets3 in c:\\users\\tamar\\anaconda3\\lib\\site-packages (0.0.11)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\tamar\\anaconda3\\lib\\site-packages (from GetOldTweets3) (4.3.4)\n",
      "Requirement already satisfied: pyquery>=1.2.10 in c:\\users\\tamar\\anaconda3\\lib\\site-packages (from GetOldTweets3) (1.4.1)\n",
      "Requirement already satisfied: cssselect>0.7.9 in c:\\users\\tamar\\anaconda3\\lib\\site-packages (from pyquery>=1.2.10->GetOldTweets3) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install GetOldTweets3\n",
    "import GetOldTweets3 as got\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+@#!?,;')\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tamar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('positive-words.txt') as f:\n",
    "    p_txt = f.read()\n",
    "    p_txt = re.sub('[,\\.()\":;!@#$%^&*\\d]|\\'s|\\'', '', p_txt)\n",
    "    p_list = p_txt.replace('\\n',' ').replace('  ',' ').lower().split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('negative-words.txt', encoding = \"ISO-8859-1\") as f:\n",
    "    n_txt = f.read()\n",
    "    n_txt = re.sub('[,\\.()\":;!@#$%^&*\\d]|\\'s|\\'', '', n_txt)\n",
    "    n_list = n_txt.replace('\\n',' ').replace('  ',' ').lower().split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Scrape Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_query_to_csv(text_query, count, startDate, endDate ):\n",
    "    \n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(text_query).setMaxTweets(count).setSince(startDate)\\\n",
    "                                           .setUntil(endDate)\n",
    "                                                    \n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "    text_tweets = [[tweet.date, tweet.text] for tweet in tweets]\n",
    "\n",
    "    tweets_df = pd.DataFrame(text_tweets, columns = ['Datetime', 'Text'])\n",
    "\n",
    "    tweets_df.to_csv('{}-tweets.csv'.format(text_query), sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropNull_low_tok(df) -> pd.DataFrame:\n",
    "    df['Text'].dropna(inplace=True)\n",
    "    df['Text'] = df['Text'].str.lower()\n",
    "    df['Text'] = df['Text'].astype(str)\n",
    "    df['Text'] = df['Text'].apply(tokenizer.tokenize)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stops(row):\n",
    "    my_list = row[\"Text\"]\n",
    "    meaningful_words = [w for w in my_list if not w in stop_words]\n",
    "    return(meaningful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_of_flat(wordlist):\n",
    "    List_flat = []\n",
    "    for i in range(len(wordlist)):\n",
    "      for j in range (len(wordlist[i])):\n",
    "        List_flat.append(wordlist[i][j])\n",
    "    return(len(List_flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count positive and negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tally(wordlist):\n",
    "    word_count_dict = {}\n",
    "    word_count_positive = {}\n",
    "    word_count_negative= {}\n",
    "\n",
    "    for tweet in range(len(wordlist)): \n",
    "        for word in wordlist[tweet]:\n",
    "            # count all words frequency\n",
    "            if word in word_count_dict.keys():\n",
    "                word_count_dict[word] += 1\n",
    "            else:\n",
    "                word_count_dict[word] = 1\n",
    "            # count if it is a positive word\n",
    "            if word in p_list:\n",
    "                if word in word_count_positive.keys():\n",
    "                    word_count_positive[word] += 1\n",
    "                else:\n",
    "                    word_count_positive[word] = 1\n",
    "            # else see if it is a negative word\n",
    "            elif word in n_list:\n",
    "                if word in word_count_negative.keys():\n",
    "                    word_count_negative[word] += 1\n",
    "                else:\n",
    "                    word_count_negative[word] = 1\n",
    "            else: \n",
    "                    pass\n",
    "    pos = sum(word_count_positive.values())\n",
    "    neg = sum(word_count_negative.values())\n",
    "    return([pos, neg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratios(pos, neg, total):\n",
    "    return((pos-neg)/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyName = \"crown holdings\"\n",
    "\n",
    "#scrape tweets\n",
    "text_query_to_csv(companyName,2500,\"2015-01-01\",\"2015-12-31\")\n",
    "x = pd.read_csv(\"C:\\\\Users\\\\Tamar\\\\Documents\\\\MMA\\\\Capstone Project\\\\\"+companyName+\"-tweets.csv\")\n",
    "\n",
    "x = dropNull_low_tok(x)\n",
    "x['Text_Clean'] = x.apply(remove_stops, axis = 1)\n",
    "x_list = x[\"Text_Clean\"].values.tolist()\n",
    "length = length_of_flat(x_list)\n",
    "\n",
    "count = tally(x_list)\n",
    "pos = count[0]\n",
    "neg = count[1]\n",
    "result = ratios(pos,neg,length)\n",
    "\n",
    "results[companyName] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ceco environmental': 0.011554160125588697,\n",
       " 'pinnacle west': 0.08140859407188386,\n",
       " 'abbott laboratories': 0.009126844512510607,\n",
       " 'aceto': 0.005400815217391304,\n",
       " 'american airlines': 0.015550376342031864,\n",
       " 'comdisco holding': 0.018137254901960786,\n",
       " 'molson coors': 0.01812979515357152,\n",
       " 'crown holdings': 0.0035293173979305367}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results.items(), columns=['Company Name', 'Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('twitterSentimentV3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
